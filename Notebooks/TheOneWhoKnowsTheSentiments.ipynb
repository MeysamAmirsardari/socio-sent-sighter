{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations:"
      ],
      "metadata": {
        "id": "OwTyM_pizVvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiL13kCEzk8m",
        "outputId": "237fa527-917e-4580-84c2-314e04c9eb01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cyVTWrappN6",
        "outputId": "1c4f9812-fc70-4b64-d010-4fc17f7e3658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports:"
      ],
      "metadata": {
        "id": "BYc9_FI1zleI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_w0JR65qoQc",
        "outputId": "f6af7d40-0717-4260-fcec-6e644792f0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "id": "RPcIwg8szoQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import copy as cp\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch_geometric.data import DataLoader, DataListLoader\n",
        "from torch_geometric.nn import DataParallel\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import global_mean_pool, GATConv"
      ],
      "metadata": {
        "id": "keQRWnHx2jvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# set the device to run on\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define the BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# define the text cleaning function\n",
        "def clean_text(text):\n",
        "    # remove URLs and Twitter handles\n",
        "    text = re.sub(r\"http\\S+|@\\S+\", \"\", text)\n",
        "    # remove non-alphanumeric characters and convert to lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).lower()\n",
        "    # tokenize the text into words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # remove stopwords\n",
        "    words = [word for word in words if word not in stopwords.words('english')]\n",
        "    # join the words back into a string\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o78zogVLTaQT",
        "outputId": "593029df-ff73-4ff5-a941-0349fdf889d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the inputs:"
      ],
      "metadata": {
        "id": "ZAzVFIAmzqlJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ZXBeS2U-vsUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04f6c83-f3ef-485e-9592-34f05bec656d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "688096910217879554\n",
            "688437279405305856\n",
            "688110059553861634\n",
            "688281442162352128\n",
            "688441130271289350\n",
            "688147918113538049\n",
            "688502260779843587\n",
            "688461902821855232\n"
          ]
        }
      ],
      "source": [
        "data_dirs = ['twitter15', 'twitter16']\n",
        "\n",
        "label_file_path = 'sentiments.txt' #'label.txt'\n",
        "source_tweets_file_path = 'source_tweets.txt'\n",
        "tree_file_dir = 'tree'\n",
        "root_path = '/content/drive/MyDrive/retweet_cascade'\n",
        "\n",
        "num_features = 1\n",
        "\n",
        "propagation_graph_list = []\n",
        "context_graph_dict = {}\n",
        "labels = []\n",
        "label_dict = {}\n",
        "\n",
        "\n",
        "for data_dir in data_dirs:\n",
        "    dir_path = os.path.join(root_path, data_dir)\n",
        "\n",
        "    with open(os.path.join(dir_path, label_file_path)) as f:\n",
        "      for line in f:\n",
        "        label = int(line.strip().split(':')[1])\n",
        "        label_dict[line.strip().split(':')[0]] = label\n",
        "\n",
        "    for file_name in os.listdir(os.path.join(dir_path, tree_file_dir)):\n",
        "        if not file_name.endswith('.txt'):\n",
        "            continue\n",
        "\n",
        "        tweet_id = file_name[:-4]\n",
        "        if len(tweet_id) > 18:\n",
        "          print(tweet_id[:18])\n",
        "          tweet_id = tweet_id[:18]\n",
        "\n",
        "        node_dict = {}\n",
        "        edge_list = []\n",
        "        index_count = 0\n",
        "        with open(os.path.join(dir_path, tree_file_dir, file_name)) as f:\n",
        "            for line in f:\n",
        "                parent, child = line.strip().split('->')\n",
        "                parent, child = parent.replace(\"'\", \"\").strip(), child.replace(\"'\", \"\").strip()\n",
        "                parent = parent.replace(\"ROOT\", str(tweet_id))\n",
        "                parent = int(parent.split(',')[1])\n",
        "                child = int(child.split(',')[1])\n",
        "\n",
        "                if parent not in node_dict:\n",
        "                    node_dict[parent] = index_count\n",
        "                    index_count += 1\n",
        "\n",
        "                if child not in node_dict:\n",
        "                    node_dict[child] = index_count\n",
        "                    index_count += 1\n",
        "\n",
        "                edge_list.append([node_dict[parent], node_dict[child]])\n",
        "\n",
        "        labels.append(label_dict[tweet_id])\n",
        "\n",
        "        # features = torch.zeros((1, num_features))\n",
        "        # with open(os.path.join(dir_path, source_tweets_file_path)) as f:\n",
        "        #     for line in f:\n",
        "        #         if line.startswith(tweet_id):\n",
        "        #             cols = line.strip().split('\\t')\n",
        "        #             features[0, 0] = int(cols[1])  # number of followers\n",
        "        #             features[0, 1] = int(cols[2])  # number of friends\n",
        "        #             features[0, 2] = float(cols[3])  # ratio of followers and friends\n",
        "        #             features[0, 3] = int(cols[4])  # number of history tweets\n",
        "        #             features[0, 4] = int(cols[5])  # registration time (year)\n",
        "        #             features[0, 5] = int(cols[6])  # whether verified account or not\n",
        "        #             break\n",
        "        features = torch.ones((1, num_features))\n",
        "\n",
        "        edge_index = torch.tensor(edge_list).t().contiguous()\n",
        "        x = features.repeat(len(edge_list) + 1, 1)\n",
        "        data = Data(x=x, edge_index=edge_index, y=label, id=tweet_id)\n",
        "\n",
        "        propagation_graph_list.append(data)\n",
        "\n",
        "    with open(os.path.join(dir_path, 'source_tweets.txt')) as f:\n",
        "      for line in f:\n",
        "        tweet_id, tweet_text = line.strip().split('\\t')\n",
        "\n",
        "        cleaned_tweet = clean_text(tweet_text)\n",
        "\n",
        "        inputs = tokenizer(cleaned_tweet, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "        outputs = model(**inputs).last_hidden_state\n",
        "\n",
        "        num_nodes = outputs.shape[1]  # number of words in the tweet\n",
        "        node_features = outputs.squeeze(0).detach().cpu().numpy()  # word embeddings\n",
        "        edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes) if i != j], dtype=torch.long).t().contiguous()  # fully-connected graph\n",
        "        data = Data(x=torch.from_numpy(node_features).float(), edge_index=edge_index)\n",
        "\n",
        "        context_graph_dict[tweet_id] = data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "context_graph_list = []\n",
        "data_list = []\n",
        "\n",
        "for graph in propagation_graph_list:\n",
        "  cont = context_graph_dict[graph.id]\n",
        "  context_graph_list.append(cont)\n",
        "\n",
        "  data_list.append(Data(\n",
        "      x_prop = graph.x,\n",
        "      edge_index_prop = graph.edge_index,\n",
        "      x_cont = cont.x,\n",
        "      edge_index_cont = cont.edge_index,\n",
        "      y = graph.y,\n",
        "      id = int(graph.id)\n",
        "      ))\n",
        "\n",
        "\n",
        "class TweetGraphDataset(Dataset):\n",
        "    def __init__(self, propagation_graphs, context_graph_list):\n",
        "        self.propagation_graphs = propagation_graphs\n",
        "        self.context_graphs = context_graph_list\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.propagation_graphs)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        propagation_graph = self.propagation_graphs[index]\n",
        "        context_graph = self.context_graphs[index]\n",
        "        return propagation_graph, context_graph\n",
        "\n",
        "dataset = TweetGraphDataset(propagation_graph_list, context_graph_list)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "WujmWn-lj3tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "torch.save(dataloader, '/content/drive/MyDrive/retweets_loader2.pth')"
      ],
      "metadata": {
        "id": "okKCUjcYHEfA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(graph_list, 'retweet_cascade_dataset.pt')"
      ],
      "metadata": {
        "id": "a5b9z1Pu705H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Craeting the Dataset:"
      ],
      "metadata": {
        "id": "VoG7eHaR1Kdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.data import Data\n",
        "\n",
        "# epoch_len = 120\n",
        "# land_mark_num = 18\n",
        "# len = df.shape[0]-1\n",
        "# overlap = 0.5\n",
        "# graph_list = []\n",
        "# order = []\n",
        "# count = 0\n",
        "# step = 10\n",
        "\n",
        "# #edge_index, edge_attr = set_edges(epoch_len, land_mark_num)\n",
        "\n",
        "# for i in range(1, len-epoch_len, step):\n",
        "#   x = set_nodes(norm_Xs, norm_Ys, CLs, i, land_mark_num, epoch_len)\n",
        "#   data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "#   graph_list.append(data)\n",
        "#   count = count + 1\n",
        "#   order.append((count, i))\n",
        "\n",
        "# graph_num = count"
      ],
      "metadata": {
        "id": "8NPG_7rD1KD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Saving the dataset:\n"
      ],
      "metadata": {
        "id": "3IQj39cZ3McO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch_geometric.data import Data\n",
        "# from torch_geometric.loader import DataLoader\n",
        "\n",
        "# loader = DataLoader(graph_list, batch_size=16)\n",
        "# torch.save(loader, '/content/drive/MyDrive/retweets_loader.pth')"
      ],
      "metadata": {
        "id": "apYBrUqV3BUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os.path as osp\n",
        "\n",
        "# import torch\n",
        "# from torch_geometric.data import Dataset, download_url\n",
        "\n",
        "# root = '/content/drive/MyDrive/Test_loader.pth'\n",
        "\n",
        "# class Dataset1(Dataset):\n",
        "#     def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
        "#         super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "#     def len(self):\n",
        "#         return len(self.processed_file_names)\n",
        "\n",
        "#     def get(self, idx):\n",
        "#         data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "#         return data"
      ],
      "metadata": {
        "id": "KmkDjaZPA2ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model:"
      ],
      "metadata": {
        "id": "Vfcx62gl3SVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* some more imports:"
      ],
      "metadata": {
        "id": "NC0qMU523ZwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, VGAE\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "GeJbZXIC3PVu"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNSentiment(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, concat=True):\n",
        "      super(GCNSentiment, self).__init__()\n",
        "\n",
        "      self.num_features = input_dim\n",
        "      self.num_classes = output_dim\n",
        "      self.nhid = hidden_dim\n",
        "      self.concat = concat\n",
        "\n",
        "      self.conv1 = GATConv(self.num_features, self.nhid * 2)\n",
        "      self.conv2 = GATConv(self.nhid * 2, self.nhid * 2)\n",
        "\n",
        "      self.fc1 = Linear(self.nhid * 2, self.nhid)\n",
        "\n",
        "      if self.concat:\n",
        "        self.fc0 = Linear(self.num_features, self.nhid)\n",
        "        self.fc1 = Linear(self.nhid * 2, self.nhid)\n",
        "      self.fc2 = Linear(self.nhid, self.num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "      x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "      x = F.selu(self.conv1(x, edge_index))\n",
        "      x = F.selu(self.conv2(x, edge_index))\n",
        "      x = F.selu(global_mean_pool(x, batch))\n",
        "      x = F.selu(self.fc1(x))\n",
        "      x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "      if self.concat:\n",
        "        news = torch.stack([data.x[(data.batch == idx).nonzero().squeeze()[0]] for idx in range(data.num_graphs)])\n",
        "        news = F.relu(self.fc0(news))\n",
        "        x = torch.cat([x, news], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "      x = F.log_softmax(self.fc2(x), dim=-1)\n",
        "      return x\n",
        "\n",
        "class CategoricalCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CategoricalCrossEntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # inputs shape: (batch_size, num_classes)\n",
        "        # targets shape: (batch_size,)\n",
        "        \n",
        "        # Convert targets to one-hot encoding\n",
        "        targets = torch.eye(inputs.size(1))[targets].to(inputs.device)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = -torch.sum(targets * torch.log_softmax(inputs, dim=1), dim=1)\n",
        "        \n",
        "        # Average loss over batch\n",
        "        loss = torch.mean(loss)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "def one_hot_ce_loss(outputs, targets):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    _, labels = torch.max(targets, dim=1)\n",
        "    return criterion(outputs, labels)"
      ],
      "metadata": {
        "id": "4Yg_7MG63dJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextEmbedding(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ContextEmbedding, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim)\n",
        "        self.gat2 = GATConv(hidden_dim, hidden_dim)\n",
        "        self.gat3 = GATConv(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.conv_mu = GCNConv(hidden_dim, output_dim)\n",
        "        # self.lin = torch.nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # x = batch.x\n",
        "        # edge_index = batch.edge_index\n",
        "\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "\n",
        "        x = self.gat2(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "\n",
        "        x = self.gat3(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "\n",
        "        x = self.conv_mu(x)\n",
        "        x = F.dropout(x, p=0.4, training=self.training)\n",
        "        z = global_mean_pool(x, batch)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PropagationEmbedding(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, concat=True):\n",
        "      super(PropagationEmbedding, self).__init__()\n",
        "\n",
        "      self.num_features = input_dim\n",
        "      self.num_classes = output_dim\n",
        "      self.nhid = hidden_dim\n",
        "      self.concat = concat\n",
        "\n",
        "      self.conv1 = GATConv(self.num_features, self.nhid * 2)\n",
        "      self.conv2 = GATConv(self.nhid * 2, self.nhid * 2)\n",
        "\n",
        "      self.fc1 = Linear(self.nhid * 2, self.nhid)\n",
        "\n",
        "      if self.concat:\n",
        "        self.fc0 = Linear(self.num_features, self.nhid)\n",
        "        self.fc1 = Linear(self.nhid * 2, self.nhid)\n",
        "      self.fc2 = Linear(self.nhid, self.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "      # x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "      x = F.selu(self.conv1(x, edge_index))\n",
        "      x = F.selu(self.conv2(x, edge_index))\n",
        "      x = F.selu(global_mean_pool(x, batch))\n",
        "      x = F.selu(self.fc1(x))\n",
        "      x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "      if self.concat:\n",
        "        news = torch.stack([data.x[(data.batch == idx).nonzero().squeeze()[0]] for idx in range(data.num_graphs)])\n",
        "        news = F.relu(self.fc0(news))\n",
        "        x = torch.cat([x, news], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "      # x = F.log_softmax(self.fc2(x), dim=-1)\n",
        "      return x\n",
        "\n",
        "class AGCNSent(torch.nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "      super(AGCNSent, self).__init__()\n",
        "      self.propagationEmb = PropagationEmbedding(1, 8, output_dim, concat=True)\n",
        "      self.contextEmb = ContextEmbedding(768, 1000, output_dim)\n",
        "      self.fc1 = Linear(output_dim * 2, 2*output_dim)\n",
        "      self.fc2 = Linear(output_dim * 2, 3)\n",
        "\n",
        "    def forward(self, data):\n",
        "      x1 = self.propagationEmb(data.x_prop, data.edge_index_prop, data)\n",
        "      x2 = self.contextEmb(data.x_cont, data.edge_index_cont, data)\n",
        "      x = torch.cat([x1, x2], dim=-1)\n",
        "      x = self.fc1(x)\n",
        "      x = self.fc2(x)\n",
        "      x = F.log_softmax(x, dim=-1)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "d3ST3drM7Ker"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.propagation_graphs[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG2KNsSf-iGT",
        "outputId": "ae1ffa9a-bfec-476e-ccbf-e5384eb76ef5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[120, 1], edge_index=[2, 119], y=1, id='522684367647559681')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model:"
      ],
      "metadata": {
        "id": "cTjFKyxP3drK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "def my_collate(batch):\n",
        "    return Batch.from_data_list(batch)"
      ],
      "metadata": {
        "id": "IHvEvDzjIIP8"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 16\n",
        "output_dim = 16\n",
        "batch_size = 16\n",
        "epoch_num = 200\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "dataloader = DataLoader(data_list, batch_size=batch_size, shuffle=True)\n",
        "# dataloader = DataLoader(data_list, batch_size=batch_size, shuffle=True, collate_fn=my_collate)\n",
        "\n",
        "model = AGCNSent(output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "criterion = CategoricalCrossEntropyLoss().to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epoch_num):\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # for batch in dataloader:\n",
        "    for batch in data_list:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch).to(device)\n",
        "\n",
        "        m = nn.Sigmoid()\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(out, batch.y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "        total_correct += (out.argmax(dim=-1) == batch.y).sum().item()\n",
        "        total_samples += batch.num_graphs\n",
        "\n",
        "    avg_loss = total_loss / 2316\n",
        "    avg_acc = total_correct / total_samples\n",
        "\n",
        "    print(\"Epoch \", epoch, \" loss: \", avg_loss, \" acc: \", avg_acc)\n",
        "    #     loss.backward()\n",
        "    #     optimizer.step()\n",
        "    #     total_loss += loss.item()\n",
        "    # print(\"Epoch \", epoch, \" loss: \", total_loss / epoch_num) # note!\n",
        "\n",
        "\n",
        "model.eval()\n",
        "pred_y = []\n",
        "true_y = []\n",
        "\n",
        "for batch in dataloader:\n",
        "    batch = batch.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model(batch)\n",
        "        pred = out.argmax(dim=-1)\n",
        "        pred_y.append(pred.cpu().numpy())\n",
        "        true_y.append(batch.y.cpu().numpy())\n",
        "\n",
        "pred_y = np.concatenate(pred_y, axis=0)\n",
        "true_y = np.concatenate(true_y, axis=0)\n",
        "\n",
        "f1 = f1_score(true_y, pred_y, average='macro')\n",
        "print(\"F1 score: \", f1)"
      ],
      "metadata": {
        "id": "gOWaWWAZ3hGx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98917ad9-ad98-4e30-9f21-5a8e1c3105c0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'max'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-87e72ad7f7a5>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-a317c38640c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagationEmb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_prop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextEmb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index_cont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-a317c38640c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_mean_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/pool/glob.py\u001b[0m in \u001b[0;36mglobal_mean_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0;34m\"dataset, remove the 'processed/' directory in the dataset's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \"root folder and try again.\")\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 f\"'{self.__class__.__name__}' object has no attribute '{key}'\")\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'max'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's watch the model!"
      ],
      "metadata": {
        "id": "l8ICm-mT_9BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m = nn.Sigmoid()\n",
        "print(m(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfERm1x2penH",
        "outputId": "0cc0b45f-52ce-4616-aa59-e59901d04053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2185, 0.3076, 0.2164],\n",
            "        [0.1923, 0.3209, 0.2245],\n",
            "        [0.1990, 0.3016, 0.2423],\n",
            "        [0.2032, 0.3138, 0.2234],\n",
            "        [0.2111, 0.2896, 0.2452],\n",
            "        [0.2056, 0.3045, 0.2328],\n",
            "        [0.2130, 0.3117, 0.2167],\n",
            "        [0.2254, 0.2895, 0.2317],\n",
            "        [0.1954, 0.3081, 0.2378],\n",
            "        [0.2032, 0.3133, 0.2240],\n",
            "        [0.2089, 0.3058, 0.2280],\n",
            "        [0.1972, 0.3051, 0.2397],\n",
            "        [0.1929, 0.3245, 0.2192],\n",
            "        [0.2162, 0.2985, 0.2300],\n",
            "        [0.2143, 0.3109, 0.2164],\n",
            "        [0.2114, 0.3045, 0.2273]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib"
      ],
      "metadata": {
        "id": "EGqIWORUA_jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(dataset):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data = dataset[0]\n",
        "        embeddings = model.conv2(F.selu(model.conv1(data.x, data.edge_index)), data.edge_index)\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "\n",
        "dataset = graph_list\n",
        "last_layer_embeddings = []\n",
        "\n",
        "for graph in dataset:\n",
        "  last_layer_embeddings.append(model.conv2(F.selu(model.conv1(data.x, data.edge_index)), data.edge_index))\n",
        "  \n",
        "# last_layer_embeddings = get_embeddings(dataset)\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "embeddings_2d = tsne.fit_transform(np.array(last_layer_embeddings))\n",
        "\n",
        "colors = ['red', 'green', 'blue']\n",
        "labels = labels\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, cmap=matplotlib.colors.ListedColormap(colors))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2s8jANYdACa4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}